{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "more_sample.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1TbXzF3s4ZA56p2MLBpyUs3B4r93MKcqT",
      "authorship_tag": "ABX9TyO1FLQ0QpgI8FzGo6yBf6SV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerry-mouse-2333/colab_save/blob/master/more_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhlS9a966VQz",
        "colab_type": "text"
      },
      "source": [
        "## ***step1. 导入必要的包***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYUH-Tuj50cR",
        "colab_type": "code",
        "outputId": "40c828ff-3a85-4fa7-ac85-e17e185fe235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import h5py\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "from functools import partial\n",
        "from tensorflow.compat.v1.keras.initializers import glorot_uniform\n",
        "\n",
        "tf.reset_default_graph()\n",
        "print('ready')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-szzfy361Zl",
        "colab_type": "text"
      },
      "source": [
        "## ***step2. 防 jupyter dead kernel***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLSQBBM6ieS",
        "colab_type": "code",
        "outputId": "13ec9e09-5cf6-4cc4-eed1-a29a45a50f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "session = InteractiveSession(config=config)\n",
        "session.close()\n",
        "\n",
        "# InteractiveSession.close(session)\n",
        "print('ready')\n",
        "## ***step3. make def for plot***\n",
        "#### **step3.1. basic 画布和标签**\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#定义子图、x轴、y轴的标签大小\n",
        "\n",
        "(plt.rcParams['axes.labelsize'],plt.rcParams['xtick.labelsize'],plt.rcParams['ytick.labelsize'])  = (14,12,12)\n",
        "\n",
        "#定义图像显示的dpi、保存的dpi，显示的窗口大小\n",
        "\n",
        "(plt.rcParams['figure.dpi'],plt.rcParams['savefig.dpi'],plt.rcParams['figure.figsize'])=(300,300,(80,60))\n",
        "\n",
        "print('ready')\n",
        "###### **step3.1.1. plot** (desired_shape:希望展示的图片大小)\n",
        "def plot_image(image, desired_shape=[128, 128]):\n",
        "    \n",
        "    plt.imshow(image.reshape(desired_shape), cmap=\"Greys\", interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "print('ready')\n",
        "#### **step3.2. where to save**\n",
        "# Where to save the figures\n",
        "\n",
        "root_dir = \"/content/drive/My Drive/3result of opts\"\n",
        "project = \"more_samples\"\n",
        "path = []\n",
        "(img_save_path,summary_save_path,ckpt_save_path) = (os.path.join(root_dir,project,\"images\"),os.path.join(root_dir,project,\"events\"),os.path.join(root_dir,project,\"models\"))\n",
        "# summary_save_path = os.path.join(root_dir,sub_dir,\"events\")\n",
        "# ckpt_save_path = os.path.join(root_dir,sub_dir,\"models\")\n",
        "path.append(img_save_path)\n",
        "path.append(summary_save_path)\n",
        "path.append(ckpt_save_path)\n",
        "for i in range(len(path)):\n",
        "  os.makedirs(path[i],exist_ok=True)\n",
        "print(path)\n",
        "\n",
        "#tight_layout: 使子图布满图像区域\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    save_name = os.path.join(img_path, fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    # os.makedirs(path, exist_ok=True)\n",
        "    \n",
        "#    if not os.path.exists(os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)):\n",
        "\n",
        "#        os.makedirs(os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID))\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(save_name, format='png', dpi=300)\n",
        "print('ready')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ready\n",
            "ready\n",
            "ready\n",
            "['/content/drive/My Drive/3result of opts/more_samples/images', '/content/drive/My Drive/3result of opts/more_samples/events', '/content/drive/My Drive/3result of opts/more_samples/models']\n",
            "ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH7XYtEV_bMs",
        "colab_type": "text"
      },
      "source": [
        "## **step4. 散度**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0rnLKeO-l7k",
        "colab_type": "code",
        "outputId": "9117810b-74b9-429d-8660-d5b59fe0d163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def kl_divergence(p, q):\n",
        "    # Kullback Leibler divergence\n",
        "    return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))\n",
        "print('ready')\n",
        "## **step4. 网络结构**\n",
        "#### **step4.1. 参数设置**\n",
        "# filters=[32,16,32]\n",
        "filters=[64,32,16]\n",
        "conv_kernel=3\n",
        "\n",
        "batch_size = 64\n",
        "ephochs = 20\n",
        "\n",
        "#原始图片尺寸\n",
        "width = 128\n",
        "height = 128\n",
        "depth = 1\n",
        "\n",
        "#输入层\n",
        "input_dim = (128,128)\n",
        "\n",
        "#编码层输出\n",
        "en_output=16\n",
        "\n",
        "#解码层输出\n",
        "output_dim = input_dim\n",
        "\n",
        "#学习率与正则参数\n",
        "learning_rate = 0.01\n",
        "l2_reg = 0.0001\n",
        "\n",
        "#散度参数\n",
        "sparsity_target = 0.1\n",
        "sparsity_weight = 0.2\n",
        "\n",
        "#train_method2：全连接层\n",
        "en1 = 300 #hid_1 of encoder\n",
        "en2 = 150  \n",
        "de1 = en2 #hid_1 of decoder\n",
        "de2 = en1\n",
        "\n",
        "\n",
        "\n",
        "print('ready')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ready\n",
            "ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chL0oLtsHZLK",
        "colab_type": "text"
      },
      "source": [
        "无奈之举，想设置kernel_size=5，strides=3，但无法使得输入输出一致"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKgwFFkEBRkZ",
        "colab_type": "text"
      },
      "source": [
        "#### **step4.2. 网络结构**\n",
        "###### **step4.21. encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L3fLIgEBbto",
        "colab_type": "code",
        "outputId": "1b85a4bb-218e-4b32-fbcf-910d88dcf1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#收集变量绘制其histogram等\n",
        "tensor_list=[]\n",
        "stds=3\n",
        "\n",
        "\n",
        "def kernel_size(na,input_channel,output_channel):\n",
        "  width = 5\n",
        "  height = 5\n",
        "  input_channel = input_channel\n",
        "  output_channel = output_channel\n",
        "  sp = [width,height,input_channel,output_channel]\n",
        "  k=tf.get_variable(name=na,shape=sp,dtype=tf.float32,initializer=glorot_uniform(),trainable=True)\n",
        "  print(k)\n",
        "  # k = tf.Variable(trainable=True,shape=shape,glorot_uniform(), dtype=tf.float32)\n",
        "\n",
        "\n",
        "  print(\"kernel_shape:\",sp)\n",
        "  return k\n",
        "# def output_size(inputs,chls,batch_size=batch_size,k=5,stds=stds):\n",
        "  \n",
        "#   width = (inputs.shape[1]-k+2)//stds+1\n",
        "#   hieght = width\n",
        "#   channels = chls\n",
        "#   return [batch_size,width,height,channels]\n",
        "\n",
        "out_shape=[]\n",
        "\n",
        "\n",
        "with tf.name_scope(\"encoder\") as scope:\n",
        "\n",
        "    def en_module(en_inputs,en_output_dim,filters,k=conv_kernel):\n",
        "        x=en_inputs\n",
        "        \n",
        "        out_shape.append([64,128,128,64])\n",
        "        i=0\n",
        "        for _ in filters:\n",
        "            print(_)\n",
        "            # x = tf.nn.conv2d(x,kernel_size(_,x.shape[-1]),output_size(x,_),stds,'conv_'+str(i))\n",
        "            #conv2d 中，没有output_shape,输入的参数为：input，kernel[4D],strides,padding)\n",
        "            # print('input_dim:',x.shape[-1])\n",
        "            x = tf.nn.conv2d(x,kernel_size('en_k_'+str(i),x.shape[-1],_),stds,[[0,0],[1,1],[1,1],[0,0]],name='conv_'+str(i))\n",
        "            tensor_list.append(x)\n",
        "            out_shape.append(x.shape)\n",
        "            # print('encoder_layer_shape',x.shape)\n",
        "            tf.add_to_collection('reuse', x)\n",
        "            x=tf.nn.leaky_relu(x, alpha=0.01,name='act_'+str(i))\n",
        "            tensor_list.append(x)\n",
        "            tf.add_to_collection('reuse', x)\n",
        "            x = tf.layers.batch_normalization(x,axis=-1,name='batch_norm_'+str(i))\n",
        "            tensor_list.append(x)\n",
        "            \n",
        "            tf.add_to_collection('reuse', x)\n",
        "            i+=1\n",
        "        \n",
        "        shape=x.get_shape().as_list()\n",
        "\n",
        "        print('out_shape:',len(out_shape))\n",
        "        flat1 = tf.layers.flatten(x,name='flattened')\n",
        "        #print(flat1.shape)\n",
        "\n",
        "        #必须使用sigmoid激活使得KL_loss!=Nan\n",
        "        half_output = my_dense_layer(flat1,en_output_dim,activation = \"sigmoid\",name=\"encoder_output\")\n",
        "\n",
        "        tf.add_to_collection('reuse', half_output)\n",
        "\n",
        "        return shape,flat1,half_output\n",
        "\n",
        "###### **step4.21. decoder**\n",
        "with tf.name_scope(\"decoder\") as scope:\n",
        "  def de_module(de_inputs,depth,filters,k=conv_kernel):\n",
        "    x = de_inputs\n",
        "    i=0\n",
        "    def o_shape(a,output_dim,bs=batch_size):\n",
        "      dim=output_dim\n",
        "      width=a[1]\n",
        "\n",
        "      return([bs,width,width,dim])\n",
        "    f=[32,64]\n",
        "    # for _ in filters[::-1]:\n",
        "    for _ in f:\n",
        "    # kernel_size函数中，由于transpose是out,in，所以此处反着输入\n",
        "        # if(i==1):\n",
        "        #   x = tf.nn.conv2d_transpose(x,kernel_size(_,x.shape[-1]),out_shape[len(out_shape)-2-i],padding=[0,1,1,1,0,0],name='conv_tran_'+str(i))\n",
        "        print(i)\n",
        "        print('本次输入：',x.shape)\n",
        "        print(\"本次目标：\",out_shape[len(out_shape)-2-i])\n",
        "                \n",
        "        idx = len(out_shape)-2-i\n",
        "        if (i==0):\n",
        "          x = tf.nn.conv2d_transpose(x,kernel_size('de_k_'+str(i),_,x.shape[-1]),o_shape(out_shape[idx],_),3,'VALID',name='conv_tran_'+str(i))\n",
        "        else:\n",
        "          x = tf.nn.conv2d_transpose(x,kernel_size('de_k_'+str(i),_,x.shape[-1]),o_shape(out_shape[idx],_),3,name='conv_tran_'+str(i))\n",
        "        print(x.shape==out_shape[len(out_shape)-2-i])\n",
        "        x=tf.nn.leaky_relu(x, alpha=0.01,name='de_act_'+str(i))\n",
        "        x = tf.layers.batch_normalization(x,axis=-1,name='de_batch_norm_'+str(i))\n",
        "        i+=1   \n",
        "      \n",
        "      #整合所有通道\n",
        "    print('给output的输入：',x.shape)\n",
        "#由于默认的stride是1，一开始报错输入的input与期待的不同，分别为42，128\n",
        "    outputs_op = tf.nn.conv2d_transpose(x,kernel_size('output_k',depth,x.shape[-1]),[64,128,128,1],3,'VALID',name='outputs_op')\n",
        "    # ,strides=3,[64,128,128,1],\n",
        "    # outputs_op = tf.layers.conv2d(x,depth,1,1,padding='same',activation=\"sigmoid\",name='outputs_op')\n",
        "    # print('output.shape:',outputs_op.shape)\n",
        "      #观察最后的重建效果\n",
        "    tf.add_to_collection('reuse', outputs_op)\n",
        "    outputs=outputs_op\n",
        "    return outputs\n",
        "\n",
        "\n",
        "###### **step4.2.3. 正式布置graph**\n",
        "# tf.reset_default_graph()\n",
        "\n",
        "inputs = tf.placeholder(tf.float32, shape=[batch_size,input_dim[0],input_dim[1],depth])#?,128,128\n",
        "print(inputs)\n",
        "#把input加入到将来要用的集合中\n",
        "tf.add_to_collection('reuse', inputs)\n",
        "print(inputs.shape)\n",
        "\n",
        "#he_init = tf.contrib.layers.variance_scaling_initializer() # He initialization\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "l2_regularizer = tf.keras.regularizers.l2(l2_reg)\n",
        "my_dense_layer = partial(tf.layers.dense,activation=tf.nn.elu,\n",
        "                             kernel_initializer=he_init,\n",
        "                             kernel_regularizer=l2_regularizer)\n",
        "#en_inputs = tf.expand_dims(inputs,axis=-1,name=\"encoder_input\")\n",
        "en_inputs=inputs\n",
        "\n",
        "shape,flat1,half_output = en_module(en_inputs,en_output_dim=en_output,filters=filters)\n",
        "print('shape_in_encoder:',shape)\n",
        "print(half_output.name)\n",
        "\n",
        "\n",
        "decode_input = my_dense_layer(half_output,np.prod(flat1.shape[1:]),name=\"decoder_input\")#现为（？，32*32*8）\n",
        "#print(decode_input)\n",
        "decode_input_reshape= tf.reshape(decode_input,shape=(-1,shape[1],shape[2],shape[3]))#how to reshape it to(?,32,32,8)\n",
        "print('decode_input.shape:',decode_input_reshape)\n",
        "outputs = de_module(decode_input_reshape,depth=depth,filters=filters)\n",
        "print(outputs)\n",
        "print(outputs.name)\n",
        "\n",
        "encode_mean = tf.reduce_mean(half_output, axis=0)\n",
        "\n",
        "## step4 可视化设置\n",
        "with tf.name_scope(\"loss\") as scope:\n",
        "    kl=kl_divergence(sparsity_target, encode_mean)\n",
        "\n",
        "    sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, encode_mean))\n",
        "    reconstruction_loss = tf.reduce_mean(tf.square(outputs - en_inputs))\n",
        "    loss = reconstruction_loss + sparsity_weight * sparsity_loss\n",
        "    #tensor_list.append(loss)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "tf.summary.scalar(\"loss\",loss)\n",
        "# 用来显示标量信息，常用于画出loss、accuracy曲线\n",
        "print(tensor_list)\n",
        "for i in range(len(tensor_list)):\n",
        "    name=tensor_list[i]\n",
        "    #print(name.dtype)\n",
        "    tf.summary.histogram(str(name),name) \n",
        "# 用来收集tensor，可显示出训练过程中变量的分布情况\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver() # not shown in the book\n",
        "print('ok')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Placeholder:0\", shape=(64, 128, 128, 1), dtype=float32)\n",
            "(64, 128, 128, 1)\n",
            "64\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "<tf.Variable 'en_k_0:0' shape=(5, 5, 1, 64) dtype=float32>\n",
            "kernel_shape: [5, 5, 1, 64]\n",
            "WARNING:tensorflow:From <ipython-input-4-608d4f32e681>:48: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/normalization.py:336: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "32\n",
            "<tf.Variable 'en_k_1:0' shape=(5, 5, 64, 32) dtype=float32>\n",
            "kernel_shape: [5, 5, 64, 32]\n",
            "16\n",
            "<tf.Variable 'en_k_2:0' shape=(5, 5, 32, 16) dtype=float32>\n",
            "kernel_shape: [5, 5, 32, 16]\n",
            "out_shape: 4\n",
            "WARNING:tensorflow:From <ipython-input-4-608d4f32e681>:57: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-608d4f32e681>:61: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "shape_in_encoder: [64, 4, 4, 16]\n",
            "encoder_output/Sigmoid:0\n",
            "decode_input.shape: Tensor(\"Reshape:0\", shape=(64, 4, 4, 16), dtype=float32)\n",
            "0\n",
            "本次输入： (64, 4, 4, 16)\n",
            "本次目标： (64, 14, 14, 32)\n",
            "<tf.Variable 'de_k_0:0' shape=(5, 5, 32, 16) dtype=float32>\n",
            "kernel_shape: [5, 5, 32, 16]\n",
            "True\n",
            "1\n",
            "本次输入： (64, 14, 14, 32)\n",
            "本次目标： (64, 42, 42, 64)\n",
            "<tf.Variable 'de_k_1:0' shape=(5, 5, 64, 32) dtype=float32>\n",
            "kernel_shape: [5, 5, 64, 32]\n",
            "True\n",
            "给output的输入： (64, 42, 42, 64)\n",
            "<tf.Variable 'output_k:0' shape=(5, 5, 1, 64) dtype=float32>\n",
            "kernel_shape: [5, 5, 1, 64]\n",
            "Tensor(\"outputs_op:0\", shape=(64, 128, 128, 1), dtype=float32)\n",
            "outputs_op:0\n",
            "[<tf.Tensor 'conv_0:0' shape=(64, 42, 42, 64) dtype=float32>, <tf.Tensor 'act_0:0' shape=(64, 42, 42, 64) dtype=float32>, <tf.Tensor 'batch_norm_0/FusedBatchNormV3:0' shape=(64, 42, 42, 64) dtype=float32>, <tf.Tensor 'conv_1:0' shape=(64, 14, 14, 32) dtype=float32>, <tf.Tensor 'act_1:0' shape=(64, 14, 14, 32) dtype=float32>, <tf.Tensor 'batch_norm_1/FusedBatchNormV3:0' shape=(64, 14, 14, 32) dtype=float32>, <tf.Tensor 'conv_2:0' shape=(64, 4, 4, 16) dtype=float32>, <tf.Tensor 'act_2:0' shape=(64, 4, 4, 16) dtype=float32>, <tf.Tensor 'batch_norm_2/FusedBatchNormV3:0' shape=(64, 4, 4, 16) dtype=float32>]\n",
            "INFO:tensorflow:Summary name Tensor(\"conv_0:0\", shape=(64, 42, 42, 64), dtype=float32) is illegal; using Tensor__conv_0_0___shape__64__42__42__64___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"act_0:0\", shape=(64, 42, 42, 64), dtype=float32) is illegal; using Tensor__act_0_0___shape__64__42__42__64___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"batch_norm_0/FusedBatchNormV3:0\", shape=(64, 42, 42, 64), dtype=float32) is illegal; using Tensor__batch_norm_0/FusedBatchNormV3_0___shape__64__42__42__64___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"conv_1:0\", shape=(64, 14, 14, 32), dtype=float32) is illegal; using Tensor__conv_1_0___shape__64__14__14__32___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"act_1:0\", shape=(64, 14, 14, 32), dtype=float32) is illegal; using Tensor__act_1_0___shape__64__14__14__32___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"batch_norm_1/FusedBatchNormV3:0\", shape=(64, 14, 14, 32), dtype=float32) is illegal; using Tensor__batch_norm_1/FusedBatchNormV3_0___shape__64__14__14__32___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"conv_2:0\", shape=(64, 4, 4, 16), dtype=float32) is illegal; using Tensor__conv_2_0___shape__64__4__4__16___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"act_2:0\", shape=(64, 4, 4, 16), dtype=float32) is illegal; using Tensor__act_2_0___shape__64__4__4__16___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"batch_norm_2/FusedBatchNormV3:0\", shape=(64, 4, 4, 16), dtype=float32) is illegal; using Tensor__batch_norm_2/FusedBatchNormV3_0___shape__64__4__4__16___dtype_float32_ instead.\n",
            "ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OJIULHcO_go",
        "colab_type": "code",
        "outputId": "6b59a90d-3187-4e47-b8e3-1a9631572c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "os.makedirs(summary_save_path, exist_ok=True)\n",
        "print(summary_save_path)\n",
        "writer= tf.summary.FileWriter(summary_save_path,tf.get_default_graph())\n",
        "#获取当前默认的保存图，并在tensorboard graph中展示\n",
        "merge_op = tf.summary.merge_all()\n",
        "#执行所有summary操作\n",
        "print(\"ready\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/3result of opts/more_samples/events\n",
            "ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzI07ImHO_NS",
        "colab_type": "text"
      },
      "source": [
        "## **step5. 开始训练与结果保存**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzCtVHyf5Vbf",
        "colab_type": "code",
        "outputId": "b15be888-a5de-456c-8d5b-677e7ce60d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# 列出所有的本地机器设备\n",
        "local_device_protos = device_lib.list_local_devices()\n",
        "# 打印\n",
        "#     print(local_device_protos)\n",
        "\n",
        "# 只打印GPU设备\n",
        "[print(x) for x in local_device_protos if x.device_type == 'GPU']\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15701401920\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 3201175561766791674\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmTFB0AlOW1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/drive/Shared drives/shared/kinds of datasets/dataset.h5'\n",
        "h5_file=h5py.File(data_path,'r')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPxgJM2ghlk5",
        "colab_type": "code",
        "outputId": "0f875efa-f9d0-46cb-f05d-2d474141d7d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train=np.array(h5_file['X_train'])\n",
        "#X_test=np.array(h5_file['X_test'])\n",
        "print(X_train.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14316, 128, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp-IljH34iBt",
        "colab_type": "code",
        "outputId": "b386884e-412c-4ef6-dfdc-78289259a9cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "print(batch_size)\n",
        "def get_batch(dataset,ephcos,batch_size):\n",
        "\n",
        "    input_queue = tf.train.slice_input_producer([dataset],num_epochs=ephochs,shuffle=True)\n",
        "\n",
        "    X_batch = tf.train.batch(input_queue, batch_size=batch_size, num_threads=1, capacity=64)\n",
        "    print(X_batch.shape)\n",
        "    return X_batch\n",
        "X_batch= get_batch(X_train,ephochs,batch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "WARNING:tensorflow:From <ipython-input-9-a01f74203bd5>:4: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:112: BaseResourceVariable.count_up_to (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Dataset.range instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From <ipython-input-9-a01f74203bd5>:6: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "(64, 128, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTXma4Y0UVWR",
        "colab_type": "code",
        "outputId": "c41a596a-373b-435f-c343-60b7c66b473c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "ckpt_name = os.path.join(ckpt_save_path,\"nnconv2d_batch64_allsamples.ckpt\")\n",
        "print(ckpt_name)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/3result of opts/more_samples/models/nnconv2d_batch64_allsamples.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXgVBpGIOPu_",
        "colab_type": "code",
        "outputId": "e30f9c66-174a-49cf-e257-2e4789dff042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    coord = tf.train.Coordinator()\n",
        "    threads = tf.train.start_queue_runners(sess, coord)\n",
        "    try:\n",
        "        for epoch in range(ephochs):\n",
        "            n_batches = X_train.shape[0] // batch_size\n",
        "\n",
        "            print(n_batches)\n",
        "            for iteration in range(n_batches):\n",
        "                # print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\") # not shown in the book\n",
        "                sys.stdout.flush()\n",
        "                x_batch=sess.run(X_batch)\n",
        "                \n",
        "                x_batch = np.expand_dims(x_batch,axis=-1)\n",
        "                # print(x_batch.shape)\n",
        "                sess.run(training_op, feed_dict={inputs: x_batch})\n",
        "                result=sess.run(merge_op, feed_dict={inputs: x_batch})\n",
        "            writer.add_summary(result,epoch)\n",
        "                \n",
        "            reconstruction_loss_val, sparsity_loss_val, loss_val = sess.run([reconstruction_loss,sparsity_loss, loss], feed_dict={inputs: x_batch})\n",
        "            #print(sess.run(kl,feed_dict={X: x_batch}))\n",
        "            print(\"\\r{}\".format(epoch), \"Train MSE:\", reconstruction_loss_val, \"\\tSparsity loss:\",sparsity_loss_val, \"\\tTotal loss:\", loss_val)           # not shown\n",
        "              \n",
        "    except tf.errors.OutOfRangeError:\n",
        "        print(\"done\")\n",
        "    finally:\n",
        "        coord.request_stop()\n",
        "    coord.join(threads)\n",
        "    saver.save(sess,ckpt_name) \n",
        "    print('ok')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-2dd0ac48b88c>:5: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "223\n",
            "0 Train MSE: 0.04337091 \tSparsity loss: 0.018707246 \tTotal loss: 0.04711236\n",
            "223\n",
            "1 Train MSE: 0.0327906 \tSparsity loss: 0.0030129403 \tTotal loss: 0.03339319\n",
            "223\n",
            "2 Train MSE: 0.026954042 \tSparsity loss: 0.002675255 \tTotal loss: 0.027489092\n",
            "223\n",
            "3 Train MSE: 0.023942614 \tSparsity loss: 0.0012972972 \tTotal loss: 0.024202073\n",
            "223\n",
            "4 Train MSE: 0.021967836 \tSparsity loss: 0.002294641 \tTotal loss: 0.022426764\n",
            "223\n",
            "5 Train MSE: 0.020657826 \tSparsity loss: 0.004872345 \tTotal loss: 0.021632295\n",
            "223\n",
            "6 Train MSE: 0.020581625 \tSparsity loss: 0.0021744273 \tTotal loss: 0.02101651\n",
            "223\n",
            "7 Train MSE: 0.018996457 \tSparsity loss: 0.004293481 \tTotal loss: 0.019855153\n",
            "223\n",
            "8 Train MSE: 0.017882545 \tSparsity loss: 0.0014921343 \tTotal loss: 0.018180972\n",
            "223\n",
            "9 Train MSE: 0.018945768 \tSparsity loss: 0.0032864003 \tTotal loss: 0.01960305\n",
            "223\n",
            "10 Train MSE: 0.017273005 \tSparsity loss: 0.0021608227 \tTotal loss: 0.01770517\n",
            "223\n",
            "11 Train MSE: 0.015090595 \tSparsity loss: 0.0010061568 \tTotal loss: 0.015291827\n",
            "223\n",
            "12 Train MSE: 0.017834978 \tSparsity loss: 0.0030639805 \tTotal loss: 0.018447774\n",
            "223\n",
            "13 Train MSE: 0.015853854 \tSparsity loss: 0.0005398158 \tTotal loss: 0.015961817\n",
            "223\n",
            "14 Train MSE: 0.016137639 \tSparsity loss: 0.0021578856 \tTotal loss: 0.016569216\n",
            "223\n",
            "15 Train MSE: 0.015398149 \tSparsity loss: 0.0032878653 \tTotal loss: 0.016055722\n",
            "223\n",
            "16 Train MSE: 0.016252367 \tSparsity loss: 0.0019436048 \tTotal loss: 0.016641088\n",
            "223\n",
            "17 Train MSE: 0.016447566 \tSparsity loss: 0.001667256 \tTotal loss: 0.016781017\n",
            "223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N-SRvK5gcPG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SevX4L3jPoIi",
        "colab_type": "text"
      },
      "source": [
        "## 1st try : \n",
        "- 1. raw_dataset(float32,255,9000张，8个G)\n",
        "-   X_batch= get_batch(X_train[0:1000],ephochs,batch_size) 报错：tensor不能大于2G\n",
        "-   ***solution:*** 只使用1000张X_train\n",
        "- 2. train 过程中OOM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmksXzgBUdWN",
        "colab_type": "text"
      },
      "source": [
        "## 2nd try:\n",
        "- to solve OOM: batch_size=32\n",
        "- 可\n",
        "- but **Nan** in summary histogram for: Tensor__conv_0/BiasAdd_0___shape__None__240__320__32___dtype_float32_\n",
        "-     try: 1. 更改学习率0.01->0.001(useless)\n",
        "        2. 更改loss为交叉熵\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrDqdtY_geJS",
        "colab_type": "text"
      },
      "source": [
        "## ***3rd: Most exciting!!!***\n",
        "- dataset：灰度化的10，000张\n",
        "- 但是仅用1000张测试，否则OOM\n",
        "- 灰度化之后，不再报 ***Nan*** 的错了！！！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tPV8pBaQmnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}