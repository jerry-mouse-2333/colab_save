{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "slove tensor2GB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1KbcFyuzDRXyT2Un4WYGkY25OvMz-SnDX",
      "authorship_tag": "ABX9TyM2RwuaoTuubiDborObzRJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerry-mouse-2333/colab_save/blob/master/slove_tensor2GB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdjOuXL3ktUb",
        "colab_type": "code",
        "outputId": "3efc93ea-329b-4f81-979a-35f4000eef58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        }
      },
      "source": [
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import h5py\n",
        "import tensorflow.compat.v1 as tf\n",
        "from functools import partial\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "session = InteractiveSession(config=config)\n",
        "session.close()\n",
        "\n",
        "# InteractiveSession.close(session)\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#定义子图、x轴、y轴的标签大小\n",
        "\n",
        "(plt.rcParams['axes.labelsize'],plt.rcParams['xtick.labelsize'],plt.rcParams['ytick.labelsize'])  = (14,12,12)\n",
        "\n",
        "#定义图像显示的dpi、保存的dpi，显示的窗口大小\n",
        "\n",
        "(plt.rcParams['figure.dpi'],plt.rcParams['savefig.dpi'],plt.rcParams['figure.figsize'])=(300,300,(80,60))\n",
        "\n",
        "\n",
        "def plot_image(image, desired_shape=[128, 128]):\n",
        "    \n",
        "    plt.imshow(image.reshape(desired_shape), cmap=\"Greys\", interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def plot_multiple_images(images, n_rows, n_cols, pad=2):\n",
        "    print(images)\n",
        "    images = images - images.min()  # make the minimum == 0, so the padding looks white\n",
        "    print(images)\n",
        "    \n",
        "    w,h = images.shape[1:]\n",
        "    \n",
        "    image = np.zeros(((w+pad)*n_rows+pad, (h+pad)*n_cols+pad))\n",
        "    \n",
        "    for y in range(n_rows):\n",
        "        for x in range(n_cols):\n",
        "            image[(y*(h+pad)+pad):(y*(h+pad)+pad+h),(x*(w+pad)+pad):(x*(w+pad)+pad+w)] = images[y*n_cols+x]\n",
        "            \n",
        "    plt.imshow(image, cmap=\"Greys\", interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# Where to save the figures\n",
        "root_dir = \"/content/drive/My Drive/result of trains\"\n",
        "\n",
        "path = os.path.join(root_dir,\"images\")\n",
        "print(path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#tight_layout: 使子图布满图像区域\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    save_path = os.path.join(path, fig_id + \".png\")\n",
        "    print(\"Saving figure to\", save_path)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    \n",
        "#    if not os.path.exists(os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)):\n",
        "\n",
        "#        os.makedirs(os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID))\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(save_name, format='png', dpi=300)\n",
        "\n",
        "# filters=[32,16,32]\n",
        "filters=[32,16]\n",
        "conv_kernel=3\n",
        "\n",
        "batch_size = 32\n",
        "ephochs = 10\n",
        "\n",
        "#原始图片尺寸\n",
        "width = 640\n",
        "height = 480\n",
        "depth = 1\n",
        "\n",
        "#输入层\n",
        "input_dim = (480,640)\n",
        "\n",
        "#编码层输出\n",
        "en_output=2\n",
        "\n",
        "#解码层输出\n",
        "output_dim = input_dim\n",
        "\n",
        "#学习率与正则参数\n",
        "learning_rate = 0.001\n",
        "l2_reg = 0.0001\n",
        "\n",
        "#散度参数\n",
        "sparsity_target = 0.2\n",
        "sparsity_weight = 0.2\n",
        "\n",
        "#train_method2：全连接层\n",
        "en1 = 300 #hid_1 of encoder\n",
        "en2 = 150  \n",
        "de1 = en2 #hid_1 of decoder\n",
        "de2 = en1\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    # Kullback Leibler divergence\n",
        "    return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))\n",
        "\n",
        "\n",
        "#收集变量绘制其histogram等\n",
        "tensor_list=[]\n",
        "\n",
        "with tf.name_scope(\"encoder\") as scope:\n",
        "\n",
        "    def en_module(en_inputs,en_output_dim,filters,k=conv_kernel):\n",
        "        x=en_inputs\n",
        "        i=0\n",
        "        for _ in filters:\n",
        "            print(_)\n",
        "            # if (i==2):\n",
        "            #   x = tf.layers.conv2d(x,_,3,2,name='conv_'+str(i))\n",
        "            # else:\n",
        "            x = tf.layers.conv2d(x,_,3,2,padding='same',name='conv_'+str(i))\n",
        "            tensor_list.append(x)\n",
        "            print(x.shape)\n",
        "            tf.add_to_collection('reuse', x)\n",
        "            x=tf.nn.leaky_relu(x, alpha=0.01,name='act_'+str(i))\n",
        "            tensor_list.append(x)\n",
        "            \n",
        "            tf.add_to_collection('reuse', x)\n",
        "            x = tf.layers.batch_normalization(x,axis=-1,name='batch_norm_'+str(i))\n",
        "            tensor_list.append(x)\n",
        "            \n",
        "            tf.add_to_collection('reuse', x)\n",
        "            i+=1\n",
        "        \n",
        "        shape=x.get_shape().as_list()\n",
        "\n",
        "        print(shape)\n",
        "        flat1 = tf.layers.flatten(x,name='flattened')\n",
        "        #print(flat1.shape)\n",
        "\n",
        "        #必须使用sigmoid激活使得KL_loss!=Nan\n",
        "        half_output = my_dense_layer(flat1,en_output_dim,activation = \"sigmoid\",name=\"encoder_output\")\n",
        "\n",
        "        tf.add_to_collection('reuse', half_output)\n",
        "\n",
        "        return shape,flat1,half_output\n",
        "\n",
        "with tf.name_scope(\"decoder\") as scope:\n",
        "  def de_module(de_inputs,depth,filters,k=conv_kernel):\n",
        "    x = de_inputs\n",
        "    i=0\n",
        "\n",
        "    for _ in filters[::-1]:\n",
        "      x = tf.layers.conv2d_transpose(x,_,3,2,padding='same',name='conv_tran_'+str(i+len(filters)))\n",
        "      print(x.shape)\n",
        "      x=tf.nn.leaky_relu(x, alpha=0.01,name='de_act_'+str(i+len(filters)))\n",
        "      \n",
        "      x = tf.layers.batch_normalization(x,axis=-1,name='de_batch_norm_'+str(i+len(filters)))\n",
        "      i+=1\n",
        "      \n",
        "\n",
        "      #整合所有通道\n",
        "    outputs_op = tf.layers.conv2d_transpose(x,depth,3,1,padding='same',activation=\"sigmoid\",name='outputs_op')\n",
        "\n",
        "      #观察最后的重建效果\n",
        "    tf.add_to_collection('reuse', outputs_op)\n",
        "    outputs=outputs_op\n",
        "    return outputs\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "inputs = tf.placeholder(tf.float32, shape=[None,input_dim[0],input_dim[1],depth])#?,128,128\n",
        "\n",
        "\n",
        "#把input加入到将来要用的集合中\n",
        "tf.add_to_collection('reuse', inputs)\n",
        "print(inputs.shape)\n",
        "\n",
        "#he_init = tf.contrib.layers.variance_scaling_initializer() # He initialization\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "l2_regularizer = tf.keras.regularizers.l2(l2_reg)\n",
        "my_dense_layer = partial(tf.layers.dense,activation=tf.nn.elu,\n",
        "                             kernel_initializer=he_init,\n",
        "                             kernel_regularizer=l2_regularizer)\n",
        "#en_inputs = tf.expand_dims(inputs,axis=-1,name=\"encoder_input\")\n",
        "en_inputs=inputs\n",
        "\n",
        "shape,flat1,half_output = en_module(en_inputs,en_output_dim=en_output,filters=filters)\n",
        "print('shape_in_encoder:',shape)\n",
        "print(half_output.name)\n",
        "\n",
        "\n",
        "decode_input = my_dense_layer(half_output,np.prod(flat1.shape[1:]),name=\"decoder_input\")#现为（？，32*32*8）\n",
        "#print(decode_input)\n",
        "decode_input_reshape= tf.reshape(decode_input,shape=(-1,shape[1],shape[2],shape[3]))#how to reshape it to(?,32,32,8)\n",
        "\n",
        "outputs = de_module(decode_input_reshape,depth=depth,filters=filters)\n",
        "print(outputs)\n",
        "print(outputs.name)\n",
        "\n",
        "encode_mean = tf.reduce_mean(half_output, axis=0)\n",
        "\n",
        "with tf.name_scope(\"loss\") as scope:\n",
        "    kl=kl_divergence(sparsity_target, encode_mean)\n",
        "\n",
        "    sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, encode_mean))\n",
        "    reconstruction_loss = tf.reduce_mean(tf.square(outputs - en_inputs))\n",
        "    loss = reconstruction_loss + sparsity_weight * sparsity_loss\n",
        "    #tensor_list.append(loss)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "tf.summary.scalar(\"loss\",loss)\n",
        "# 用来显示标量信息，常用于画出loss、accuracy曲线\n",
        "print(tensor_list)\n",
        "for i in range(len(tensor_list)):\n",
        "    name=tensor_list[i]\n",
        "    #print(name.dtype)\n",
        "    tf.summary.histogram(str(name),name) \n",
        "# 用来收集tensor，可显示出训练过程中变量的分布情况\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver() # not shown in the book\n",
        "\n",
        "\n",
        "summary_save_dir = os.path.join(root_dir,\"models\")\n",
        "print(summary_save_dir)\n",
        "\n",
        "os.makedirs(summary_save_dir, exist_ok=True)\n",
        "print(summary_save_dir )\n",
        "writer= tf.summary.FileWriter(summary_save_dir,tf.get_default_graph())\n",
        "#获取当前默认的保存图，并在tensorboard graph中展示\n",
        "\n",
        "\n",
        "merge_op = tf.summary.merge_all()\n",
        "#执行所有summary操作\n",
        "print(\"end of before\")\n",
        "\n",
        "data_path = '/content/drive/Shared drives/shared/train_gray.h5'\n",
        "h5_file=h5py.File(data_path,'r')\n",
        "\n",
        "X_train=np.array(h5_file['X_train'])[0:5000]\n",
        "# [0:5000]\n",
        "#X_test=np.array(h5_file['X_test'])\n",
        "print(X_train.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 480, 640, 1)\n",
            "32\n",
            "WARNING:tensorflow:From <ipython-input-1-c4a410e40a3c>:132: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "(None, 240, 320, 32)\n",
            "WARNING:tensorflow:From <ipython-input-1-c4a410e40a3c>:140: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "16\n",
            "(None, 120, 160, 16)\n",
            "[None, 120, 160, 16]\n",
            "WARNING:tensorflow:From <ipython-input-1-c4a410e40a3c>:149: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-c4a410e40a3c>:153: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "shape_in_encoder: [None, 120, 160, 16]\n",
            "encoder_output/Sigmoid:0\n",
            "WARNING:tensorflow:From <ipython-input-1-c4a410e40a3c>:165: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "(None, 240, 320, 16)\n",
            "(None, 480, 640, 32)\n",
            "Tensor(\"outputs_op/Sigmoid:0\", shape=(None, 480, 640, 1), dtype=float32)\n",
            "outputs_op/Sigmoid:0\n",
            "[<tf.Tensor 'conv_0/BiasAdd:0' shape=(None, 240, 320, 32) dtype=float32>, <tf.Tensor 'act_0:0' shape=(None, 240, 320, 32) dtype=float32>, <tf.Tensor 'batch_norm_0/FusedBatchNormV3:0' shape=(None, 240, 320, 32) dtype=float32>, <tf.Tensor 'conv_1/BiasAdd:0' shape=(None, 120, 160, 16) dtype=float32>, <tf.Tensor 'act_1:0' shape=(None, 120, 160, 16) dtype=float32>, <tf.Tensor 'batch_norm_1/FusedBatchNormV3:0' shape=(None, 120, 160, 16) dtype=float32>]\n",
            "INFO:tensorflow:Summary name Tensor(\"conv_0/BiasAdd:0\", shape=(None, 240, 320, 32), dtype=float32) is illegal; using Tensor__conv_0/BiasAdd_0___shape__None__240__320__32___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"act_0:0\", shape=(None, 240, 320, 32), dtype=float32) is illegal; using Tensor__act_0_0___shape__None__240__320__32___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"batch_norm_0/FusedBatchNormV3:0\", shape=(None, 240, 320, 32), dtype=float32) is illegal; using Tensor__batch_norm_0/FusedBatchNormV3_0___shape__None__240__320__32___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"conv_1/BiasAdd:0\", shape=(None, 120, 160, 16), dtype=float32) is illegal; using Tensor__conv_1/BiasAdd_0___shape__None__120__160__16___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"act_1:0\", shape=(None, 120, 160, 16), dtype=float32) is illegal; using Tensor__act_1_0___shape__None__120__160__16___dtype_float32_ instead.\n",
            "INFO:tensorflow:Summary name Tensor(\"batch_norm_1/FusedBatchNormV3:0\", shape=(None, 120, 160, 16), dtype=float32) is illegal; using Tensor__batch_norm_1/FusedBatchNormV3_0___shape__None__120__160__16___dtype_float32_ instead.\n",
            "/My Drive/result of trains/models\n",
            "end of before\n",
            "(5000, 480, 640)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXDx5RYIJBHB",
        "colab_type": "code",
        "outputId": "07cec127-3de3-414e-daea-469bb3b82714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8) \n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True, gpu_options=gpu_options))\n",
        "def get_batch(dataset,ephochs,batch_size):\n",
        "    print('o')\n",
        "    with tf.variable_scope('shabi',reuse=tf.AUTO_REUSE):\n",
        "      d_plhd = tf.placeholder(dtype=tf.float32, shape=(None,480,640))  #定义一个placeholder  \n",
        "      dl = tf.get_variable('d1',[5000,480,640])   #再定义一个变量\n",
        "      print(dl.name)\n",
        "    # print('h')\n",
        "      with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.local_variables_initializer())\n",
        "        sess.run(dl.assign(d_plhd), {d_plhd: dataset})\n",
        "      # print sess.run(tf.reduce_sum(w))++++++++\n",
        "    # print('y')\n",
        "        input_queue = tf.train.slice_input_producer([dl],num_epochs=ephochs,shuffle=True)\n",
        "    # with tf.Session() as sess:\n",
        "    #     sess.run(tf.global_variables_initializer())\n",
        "    #     sess.run(tf.local_variables_initializer())\n",
        "    #     coord = tf.train.Coordinator()\n",
        "    #     threads = tf.train.start_queue_runners(sess, coord)\n",
        "        \n",
        "    #     try:\n",
        "    #       for _ in range(1):\n",
        "    #         print(_)\n",
        "    #         print(dataset.shape)\n",
        "    #         sess.run(input_queue_op,feed_dict={dataset_0:dataset})\n",
        "\n",
        "    #         # print('ok')\n",
        "    #     except tf.errors.OutOfRangeError:\n",
        "    #         print(\"done\")\n",
        "    #     finally:\n",
        "    #         coord.request_stop()\n",
        "    #     coord.join(threads)\n",
        "        print('ohye')\n",
        "        X_batch = tf.train.batch(input_queue, batch_size=batch_size, num_threads=1, capacity=64)\n",
        "\n",
        "    print(X_batch.shape)\n",
        "    return X_batch\n",
        "X_batch= get_batch(X_train,ephochs,batch_size)\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    coord = tf.train.Coordinator()\n",
        "    threads = tf.train.start_queue_runners(sess, coord)\n",
        "    try:\n",
        "        for epoch in range(ephochs):\n",
        "            n_batches = X_train.shape[0] // batch_size\n",
        "            print(n_batches)\n",
        "            for iteration in range(n_batches):\n",
        "                # print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\") # not shown in the book\n",
        "                sys.stdout.flush()\n",
        "                x_batch=sess.run(X_batch)\n",
        "                x_batch = np.expand_dims(x_batch,axis=-1)\n",
        "                # print(x_batch.shape)\n",
        "                sess.run(training_op, feed_dict={inputs: x_batch})\n",
        "                result=sess.run(merge_op, feed_dict={inputs: x_batch})\n",
        "                writer.add_summary(result,iteration)\n",
        "                \n",
        "            reconstruction_loss_val, sparsity_loss_val, loss_val = sess.run([reconstruction_loss,sparsity_loss, loss], feed_dict={inputs: x_batch})\n",
        "            #print(sess.run(kl,feed_dict={X: x_batch}))\n",
        "            print(\"\\r{}\".format(epoch), \"Train MSE:\", reconstruction_loss_val, \"\\tSparsity loss:\",sparsity_loss_val, \"\\tTotal loss:\", loss_val)           # not shown\n",
        "              \n",
        "    except tf.errors.OutOfRangeError:\n",
        "        print(\"done\")\n",
        "    finally:\n",
        "        coord.request_stop()\n",
        "    coord.join(threads)\n",
        "    \n",
        "    save_dir = '/content/drive/My Drive/result of trains'\n",
        "    print(save_dir)\n",
        "    save_path = os.path.join(save_dir,\"my_model_gray_5000_sparse_2.ckpt\")\n",
        "\n",
        "    saver.save(sess, save_path) \n",
        "    print('ok')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "\n",
            "o\n",
            "shabi/d1:0\n",
            "WARNING:tensorflow:From <ipython-input-3-8d61d845c073>:16: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:112: BaseResourceVariable.count_up_to (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Dataset.range instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "ohye\n",
            "WARNING:tensorflow:From <ipython-input-3-8d61d845c073>:36: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "(32, 480, 640)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:235: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-8d61d845c073>:46: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "156\n",
            "0 Train MSE: 0.00011120071 \tSparsity loss: 1.198845e-06 \tTotal loss: 0.00011144048\n",
            "156\n",
            "1 Train MSE: 3.794971e-05 \tSparsity loss: 1.5745172e-07 \tTotal loss: 3.79812e-05\n",
            "156\n",
            "2 Train MSE: 1.7968225e-05 \tSparsity loss: -7.403287e-09 \tTotal loss: 1.7966744e-05\n",
            "156\n",
            "3 Train MSE: 1.0257695e-05 \tSparsity loss: 9.386804e-08 \tTotal loss: 1.0276469e-05\n",
            "156\n",
            "4 Train MSE: 6.5781896e-06 \tSparsity loss: -7.7117875e-08 \tTotal loss: 6.562766e-06\n",
            "156\n",
            "5 Train MSE: 4.62337e-06 \tSparsity loss: 8.0397585e-06 \tTotal loss: 6.2313215e-06\n",
            "156\n",
            "6 Train MSE: 3.3715821e-06 \tSparsity loss: 1.0422082e-07 \tTotal loss: 3.3924264e-06\n",
            "156\n",
            "7 Train MSE: 2.6053517e-06 \tSparsity loss: 2.0084553e-07 \tTotal loss: 2.645521e-06\n",
            "156\n",
            "8 Train MSE: 2.4559106e-06 \tSparsity loss: 0.0023461003 \tTotal loss: 0.00047167597\n",
            "156\n",
            "9 Train MSE: 1.7131725e-06 \tSparsity loss: 6.8894224e-08 \tTotal loss: 1.7269514e-06\n",
            "ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AAGjLQ4EoAm",
        "colab_type": "code",
        "outputId": "8207a681-97ee-4e76-e6d1-9d83a97296ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# 列出所有的本地机器设备\n",
        "local_device_protos = device_lib.list_local_devices()\n",
        "# 打印\n",
        "#     print(local_device_protos)\n",
        "\n",
        "# 只打印GPU设备\n",
        "[print(x) for x in local_device_protos if x.device_type == 'GPU']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15701463552\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 1545449782497483954\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iex8yUllmaq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6ce1865e-730f-4949-cb02-59c35b9fd1cb"
      },
      "source": [
        "root_dir = \"/content/drive/My Drive/result of trains\"\n",
        "sub_dir = \"/My Drive/result of trains\"\n",
        "# path = os.path.join(root_dir,sub_dir,\"images\")\n",
        "path = os.path.join(root_dir,\"models\")\n",
        "print(path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/result of trains/models\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}